import nltk #NLTKæ˜¯ä¸€ä¸ªæµè¡Œçš„è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰åº“ï¼Œæä¾›äº†è®¸å¤šç”¨äºå¤„ç†æ–‡æœ¬å’Œæ‰§è¡Œå„ç§NLPä»»åŠ¡çš„åŠŸèƒ½å’Œå·¥å…·ã€‚
import gradio as gr #Gradioæ˜¯ä¸€ä¸ªç”¨äºæ„å»ºäº¤äº’å¼ç•Œé¢çš„Python
import models.shared as shared
from configs.model_config import * #å¯¼å…¥é…ç½®æ–‡ä»¶å‚æ•°
from chains.local_doc_qa import LocalDocQA#å¯¼å…¥äº†ä¸€ä¸ªåä¸ºLocalDocQAçš„ç±»
from models.loader.args import parser#å¯¼å…¥å‚æ•°
from models.loader import LoaderCheckPoint
# NLTK_DATA_PATH='/home/zzu_zxw/zjl_data/ChatGLM-6B/langchain-ChatGLM/nltk_data'
# nltk.data.path = [NLTK_DATA_PATH] + nltk.data.path

args = None
args = parser.parse_args()#è§£æå‘½ä»¤è¡Œå‚æ•°
args_dict = vars(args)#å‚æ•°è½¬ä¸ºå­—å…¸å½¢å¼
shared.loaderCheckPoint = LoaderCheckPoint(args_dict)#åŠ è½½è‡ªå®šä¹‰ model CheckPoint
llm_model_ins = shared.loaderLLM()  # åŠ è½½æ¨¡å‹  #ç”¨äºåˆå§‹åŒ–å¹¶è¿”å›LLMçš„å®ä¾‹å¯¹è±¡ã€‚
llm_model_ins.history_len = LLM_HISTORY_LEN
local_doc_qa = LocalDocQA()# åˆ›å»ºå®ä¾‹local_doc_qaï¼ŒLocalDocQA æ˜¯ä¸€ä¸ªæœ¬åœ°æ–‡æ¡£é—®ç­”æ¨¡å‹çš„ç±»ã€‚è¿™ä¸ªç±»å°è£…äº†é—®ç­”ç³»ç»Ÿçš„æ ¸å¿ƒé€»è¾‘ï¼ŒåŒ…æ‹¬æ–‡æ¡£åŠ è½½ã€åµŒå…¥æ¨¡å‹å¤„ç†ã€ç›¸ä¼¼æ€§æœç´¢ã€æç¤ºç”Ÿæˆå’Œç­”æ¡ˆç”Ÿæˆç­‰åŠŸèƒ½ï¼Œä»¥æä¾›åŸºäºæœ¬åœ°æ–‡æ¡£çš„é—®ç­”æœåŠ¡ã€‚
local_doc_qa.init_cfg(llm_model=llm_model_ins, #llm_modelæŒ‡å®šçš„è¯­è¨€æ¨¡å‹å¯¹è±¡ï¼Œå³ç”¨äºå›ç­”é—®é¢˜çš„è¯­è¨€æ¨¡å‹
                      embedding_model=EMBEDDING_MODEL,#æŒ‡å®šçš„åµŒå…¥æ¨¡å‹åç§°ï¼Œç”¨äºå°†æ–‡æ¡£è½¬æ¢ä¸ºå‘é‡è¡¨ç¤ºã€‚ä¸ºï¼š'text2vec'
                      embedding_device=EMBEDDING_DEVICE,#æŒ‡å®šçš„åµŒå…¥æ¨¡å‹è®¾å¤‡ï¼Œå³å°†åµŒå…¥æ¨¡å‹åŠ è½½åˆ°å“ªä¸ªè®¾å¤‡ä¸Šè¿›è¡Œè®¡ç®—ã€‚
                      top_k=VECTOR_SEARCH_TOP_K)#æŒ‡å®šçš„ç›¸ä¼¼æ€§æœç´¢ç»“æœè¿”å›çš„å‰ K ä¸ªæ–‡æ¡£ã€‚

# filepath = "content/zhuanzhi/triple_zhuti_lunwen.txt"
filepath = "content/zhuanzhi/ID"
# filepath = "content/zhuanzhi/aaa.txt"
#è¯¥æ–¹æ³•çš„ä½œç”¨æ˜¯ä»æ–‡ä»¶åŠ è½½ä¸€ç»„æ–‡æ¡£ï¼Œå¹¶ä½¿ç”¨FAISSåº“åˆ›å»ºä¸€ä¸ªå‘é‡å­˜å‚¨ã€‚å‘é‡å­˜å‚¨ç”¨äºåŸºäºæ–‡æ¡£åµŒå…¥çš„é«˜æ•ˆç›¸ä¼¼æ€§æœç´¢ã€‚

#init_knowledge_vector_storeæ–¹æ³•è¿”å›ä¸€ä¸ªå‘é‡å­˜å‚¨çš„è·¯å¾„vs_pathï¼Œ
#ä»¥åŠä¸€ä¸ªç©ºåˆ—è¡¨ï¼ˆç”±äºåœ¨ä»£ç ä¸­ä½¿ç”¨äº†å ä½ç¬¦_ï¼Œè¡¨ç¤ºä¸éœ€è¦æ¥æ”¶è¯¥è¿”å›å€¼ï¼‰ã€‚
#ä½ å¯ä»¥ä½¿ç”¨è¿”å›çš„å‘é‡å­˜å‚¨è·¯å¾„vs_pathæ¥è¿›è¡Œåç»­çš„ç›¸ä¼¼æ€§æœç´¢æ“ä½œæˆ–å…¶ä»–ç›¸å…³æ“ä½œã€‚
# vs_path = "/home/zzu_zxw/zjl_data/ChatGLM-6B/langchain-ChatGLM/vector_store/TopicID_FAISS_20230706_182112"#æ–‡ç« åŠ ä¸»é¢˜åŠ æ¨è
# vs_path = "/home/zzu_zxw/zjl_data/ChatGLM-6B/langchain-ChatGLM/vector_store/TopicID_FAISS_20230707_155126"#æ–‡ç« æœ‰èµ„æºé“¾æ¥å’Œä¸‹è½½é“¾æ¥
vs_path = "/home/zzu_zxw/zjl_data/ChatGLM-6B/langchain-ChatGLM/vector_store/TopicID_FAISS_20230707_215737"
# vs_path, _ = local_doc_qa.init_knowledge_vector_store(filepath,vs_path)
# vs_path, _ = local_doc_qa.init_knowledge_vector_store(filepath)


# åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
# from sentence_transformers import SentenceTransformer
# model = SentenceTransformer('./paraphrase-multilingual-MiniLM-L12-v2/')
def get_answer(query,  history,streaming: bool = STREAMING):
        for response, history in local_doc_qa.get_knowledge_based_answer(
                query=query, vs_path=vs_path, chat_history=history, streaming=streaming):

            # import html
            # prompt = response["source_documents"]
            # prompt_list = prompt.split("}")[:-1]
            # prompt_content = "".join([
            #     f"<details><summary>Source [{i + 1}]</summary><pre>{html.escape(doc)}</pre></details>"
            #     for i, doc in enumerate(prompt_list)
            # ])
            # history[-1][-1] += prompt_content

            import html
            prompt = response["source_documents"]
            prompt_content = f"<details><summary>Prompt</summary><pre>{html.escape(prompt)}</pre></details>"
            history[-1][-1] += prompt_content

            yield history, ""

        logger.info(f"flagging: username={FLAG_USER_NAME},query={query},vs_path={vs_path},history={history}")#æ‰“å°åˆ°æ§åˆ¶å°


block_css = """.importantButton {
    background: linear-gradient(45deg, #7e0570,#5d1c99, #6e00ff) !important;
    border: none !important;
}
.importantButton:hover {
    background: linear-gradient(45deg, #ff00e0,#8500ff, #6e00ff) !important;
    border: none !important;
}"""

webui_title = """
# ğŸ‰ChatZhuanzhiğŸ‰
"""
init_message = f"""æ¬¢è¿ä½¿ç”¨ ChatZhuanzhiï¼"""

default_theme_args = dict(
    font=["Source Sans Pro", 'ui-sans-serif', 'system-ui', 'sans-serif'],
    font_mono=['IBM Plex Mono', 'ui-monospace', 'Consolas', 'monospace'],
)

with gr.Blocks(css=block_css, theme=gr.themes.Default(**default_theme_args)) as demo:
    gr.Markdown(webui_title)
    with gr.Tab("å¯¹è¯"):
        with gr.Row():
            with gr.Column(scale=10):
                chatbot = gr.Chatbot([[None, init_message]],
                                     elem_id="chat-box",
                                     show_label=False).style(height=600)
                query = gr.Textbox(show_label=False,
                                   placeholder="è¯·è¾“å…¥æé—®å†…å®¹ï¼ŒæŒ‰å›è½¦è¿›è¡Œæäº¤").style(container=False)

                query.submit(get_answer,
                             [query, chatbot],
                             [chatbot, query])
    demo.load(
        inputs=None,
        outputs=[],
        queue=True,
        show_progress=False,
    )

(demo
 .queue(concurrency_count=3)
 .launch(server_name='127.0.0.1',
         server_port=8888,
         show_api=False,
         share=True,
         inbrowser=False))
